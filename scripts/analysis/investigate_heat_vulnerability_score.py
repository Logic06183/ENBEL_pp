#!/usr/bin/env python3
"""
Investigation of HEAT_VULNERABILITY_SCORE Dominance
==================================================

Deep dive into why HEAT_VULNERABILITY_SCORE dominates predictions:
1. Analyze its construction and components
2. Check for potential data leakage
3. Examine its relationship with biomarkers
4. Validate its clinical relevance
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

print("=" * 70)
print("HEAT_VULNERABILITY_SCORE INVESTIGATION")
print("=" * 70)

# Load data
clinical_df = pd.read_csv('data/raw/CLINICAL_DATASET_COMPLETE_CLIMATE.csv', low_memory=False)
print(f"Loaded clinical data: {clinical_df.shape}")

# Check if HEAT_VULNERABILITY_SCORE exists
if 'HEAT_VULNERABILITY_SCORE' not in clinical_df.columns:
    print("‚ùå HEAT_VULNERABILITY_SCORE not found in dataset")
    exit()

print(f"\n{'='*50}")
print("1. HEAT_VULNERABILITY_SCORE BASIC ANALYSIS")
print("="*50)

heat_vuln = clinical_df['HEAT_VULNERABILITY_SCORE'].dropna()
print(f"HEAT_VULNERABILITY_SCORE data:")
print(f"  Available samples: {len(heat_vuln):,} ({len(heat_vuln)/len(clinical_df)*100:.1f}%)")
print(f"  Mean: {heat_vuln.mean():.2f}")
print(f"  Median: {heat_vuln.median():.2f}")
print(f"  Range: {heat_vuln.min():.2f} - {heat_vuln.max():.2f}")
print(f"  Std deviation: {heat_vuln.std():.2f}")

# Check distribution
print(f"\nDistribution analysis:")
print(f"  Unique values: {heat_vuln.nunique()}")
print(f"  Skewness: {stats.skew(heat_vuln):.3f}")
print(f"  Kurtosis: {stats.kurtosis(heat_vuln):.3f}")

# Value distribution
percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]
print(f"\nPercentile distribution:")
for p in percentiles:
    val = heat_vuln.quantile(p/100)
    print(f"  {p:2d}th percentile: {val:6.2f}")

# Check for suspicious patterns
print(f"\n{'='*50}")
print("2. SUSPICIOUS PATTERN DETECTION")
print("="*50)

# Check for perfect scores (100%)
perfect_scores = (heat_vuln == 100).sum()
zero_scores = (heat_vuln == 0).sum()
print(f"Perfect vulnerability scores (100): {perfect_scores} ({perfect_scores/len(heat_vuln)*100:.1f}%)")
print(f"Zero vulnerability scores (0): {zero_scores} ({zero_scores/len(heat_vuln)*100:.1f}%)")

if perfect_scores > len(heat_vuln) * 0.1:
    print("‚ö†Ô∏è  High proportion of perfect scores - possible artificial construction")

# Check for discrete vs continuous distribution
value_counts = heat_vuln.value_counts().head(10)
print(f"\nMost common values:")
for value, count in value_counts.items():
    pct = (count / len(heat_vuln)) * 100
    print(f"  {value:6.2f}: {count:5} occurrences ({pct:4.1f}%)")

# Check if it's artificially discrete
if heat_vuln.nunique() < 20:
    print("‚ö†Ô∏è  Very few unique values - likely artificially constructed")

print(f"\n{'='*50}")
print("3. RELATIONSHIP WITH OTHER VARIABLES")
print("="*50)

# Check relationships with biomarkers
biomarkers = [
    'CD4 cell count (cells/¬µL)',
    'total_cholesterol_mg_dL', 
    'creatinine_umol_L',
    'fasting_glucose_mmol_L',
    'hemoglobin_g_dL'
]

# Find available biomarkers
available_biomarkers = [b for b in biomarkers if b in clinical_df.columns]
print(f"Testing correlations with {len(available_biomarkers)} biomarkers:")

biomarker_correlations = []
for biomarker in available_biomarkers:
    # Get complete cases
    pair_data = clinical_df[['HEAT_VULNERABILITY_SCORE', biomarker]].dropna()
    
    if len(pair_data) > 100:
        corr = pair_data['HEAT_VULNERABILITY_SCORE'].corr(pair_data[biomarker])
        p_value = stats.pearsonr(pair_data['HEAT_VULNERABILITY_SCORE'], pair_data[biomarker])[1]
        
        biomarker_correlations.append((biomarker, corr, p_value, len(pair_data)))
        
        warning = "üö®" if abs(corr) > 0.7 else "‚ö†Ô∏è" if abs(corr) > 0.5 else ""
        print(f"  {biomarker:<35} r = {corr:6.3f} (p = {p_value:.6f}, n = {len(pair_data)}) {warning}")

# Sort by correlation strength
biomarker_correlations.sort(key=lambda x: abs(x[1]), reverse=True)

print(f"\nStrongest biomarker correlations:")
for biomarker, corr, p_val, n in biomarker_correlations[:3]:
    print(f"  {biomarker}: r = {corr:.3f}")

print(f"\n{'='*50}")
print("4. COMPONENT ANALYSIS")
print("="*50)

# Try to identify potential components of HEAT_VULNERABILITY_SCORE
# Look for variables that might be used in its construction

candidate_components = []

# Climate variables that might be components
climate_patterns = ['temp', 'humid', 'heat', 'climate']
for pattern in climate_patterns:
    matching = [col for col in clinical_df.columns if pattern in col.lower()]
    candidate_components.extend(matching)

# Demographic variables
demographic_patterns = ['age', 'sex', 'race', 'income', 'education', 'dwelling']
for pattern in demographic_patterns:
    matching = [col for col in clinical_df.columns if pattern in col.lower()]
    candidate_components.extend(matching)

# Remove duplicates and filter for reasonable coverage
candidate_components = list(set(candidate_components))
candidate_components = [col for col in candidate_components 
                       if col in clinical_df.columns 
                       and clinical_df[col].notna().sum() > len(clinical_df) * 0.1]

print(f"Potential component variables ({len(candidate_components)}):")

component_correlations = []
for component in candidate_components:
    if clinical_df[component].dtype in [np.float64, np.int64]:
        # Numeric variable
        pair_data = clinical_df[['HEAT_VULNERABILITY_SCORE', component]].dropna()
        
        if len(pair_data) > 100:
            corr = stats.pearsonr(pair_data['HEAT_VULNERABILITY_SCORE'], pair_data[component])[0]
            if not pd.isna(corr):
                component_correlations.append((component, abs(corr), corr))

# Sort by absolute correlation
component_correlations.sort(key=lambda x: x[1], reverse=True)

print(f"\nTop 10 potential components by correlation:")
for component, abs_corr, corr in component_correlations[:10]:
    warning = "üö®" if abs_corr > 0.8 else "‚ö†Ô∏è" if abs_corr > 0.6 else ""
    print(f"  {component:<35} |r| = {abs_corr:.3f} (r = {corr:6.3f}) {warning}")

print(f"\n{'='*50}")
print("5. DATA LEAKAGE INVESTIGATION")
print("="*50)

# Check for perfect or near-perfect correlations that suggest data leakage
print("Checking for data leakage indicators:")

very_high_corr = [x for x in component_correlations if x[1] > 0.9]
if very_high_corr:
    print(f"‚ö†Ô∏è  Variables with |r| > 0.9:")
    for component, abs_corr, corr in very_high_corr:
        print(f"    {component}: r = {corr:.3f}")
    print("    ‚Üí Possible data leakage or circular construction")

# Check temporal consistency
if 'primary_date' in clinical_df.columns:
    print(f"\nTemporal analysis:")
    
    # Convert date and check HEAT_VULNERABILITY_SCORE over time
    date_vuln_data = clinical_df[['primary_date', 'HEAT_VULNERABILITY_SCORE']].dropna()
    date_vuln_data['date'] = pd.to_datetime(date_vuln_data['primary_date'], errors='coerce')
    date_vuln_data = date_vuln_data.dropna()
    
    if len(date_vuln_data) > 100:
        date_vuln_data['year'] = date_vuln_data['date'].dt.year
        date_vuln_data['month'] = date_vuln_data['date'].dt.month
        
        # Check yearly variation
        yearly_stats = date_vuln_data.groupby('year')['HEAT_VULNERABILITY_SCORE'].agg(['mean', 'std', 'count'])
        print(f"  Yearly variation:")
        for year, row in yearly_stats.iterrows():
            if row['count'] > 10:
                print(f"    {year}: mean = {row['mean']:.2f}, std = {row['std']:.2f} (n = {row['count']})")
        
        # Check if scores are suspiciously constant over time
        yearly_variation = yearly_stats['mean'].std()
        print(f"  Yearly variation (std of means): {yearly_variation:.3f}")
        
        if yearly_variation < 5:
            print("  ‚ö†Ô∏è  Low temporal variation - scores may be static/artificial")

print(f"\n{'='*50}")
print("6. CLINICAL RELEVANCE ASSESSMENT")
print("="*50)

# Check if HEAT_VULNERABILITY_SCORE makes clinical sense
print("Assessing clinical relevance:")

# Relationship with age (vulnerable populations typically older)
if any('age' in col.lower() for col in clinical_df.columns):
    age_cols = [col for col in clinical_df.columns if 'age' in col.lower()]
    age_col = age_cols[0]
    
    age_vuln_data = clinical_df[[age_col, 'HEAT_VULNERABILITY_SCORE']].dropna()
    if len(age_vuln_data) > 100:
        age_corr = age_vuln_data[age_col].corr(age_vuln_data['HEAT_VULNERABILITY_SCORE'])
        print(f"  Age correlation: r = {age_corr:.3f}")
        
        if age_corr > 0.2:
            print("    ‚úÖ Positive correlation with age (expected for vulnerability)")
        elif age_corr < -0.2:
            print("    ‚ö†Ô∏è  Negative correlation with age (unexpected)")
        else:
            print("    ‚ö†Ô∏è  Weak correlation with age")

# Check seasonal patterns (heat vulnerability should vary by season)
if 'month' in clinical_df.columns or any('season' in col.lower() for col in clinical_df.columns):
    print(f"\n  Seasonal analysis:")
    
    if 'month' in clinical_df.columns:
        monthly_vuln = clinical_df.groupby('month')['HEAT_VULNERABILITY_SCORE'].agg(['mean', 'count'])
        
        print(f"    Monthly HEAT_VULNERABILITY_SCORE patterns:")
        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
        
        for month_num, row in monthly_vuln.iterrows():
            if row['count'] > 10:
                month_name = months[int(month_num)-1] if 1 <= month_num <= 12 else str(month_num)
                print(f"      {month_name}: {row['mean']:.2f} (n = {row['count']})")
        
        # Check if summer months have higher vulnerability
        if len(monthly_vuln) >= 12:
            summer_months = monthly_vuln.loc[monthly_vuln.index.isin([12, 1, 2])]  # Dec, Jan, Feb (Southern Hemisphere)
            winter_months = monthly_vuln.loc[monthly_vuln.index.isin([6, 7, 8])]   # Jun, Jul, Aug
            
            if len(summer_months) > 0 and len(winter_months) > 0:
                summer_mean = summer_months['mean'].mean()
                winter_mean = winter_months['mean'].mean()
                
                print(f"    Summer (Dec-Feb) mean: {summer_mean:.2f}")
                print(f"    Winter (Jun-Aug) mean: {winter_mean:.2f}")
                
                if summer_mean > winter_mean:
                    print("    ‚úÖ Higher vulnerability in summer (expected)")
                else:
                    print("    ‚ö†Ô∏è  Higher vulnerability in winter (unexpected for heat)")

print(f"\n{'='*50}")
print("7. SUMMARY AND RECOMMENDATIONS")
print("="*50)

print(f"HEAT_VULNERABILITY_SCORE Investigation Summary:")
print(f"‚úì Data availability: {len(heat_vuln):,} samples")
print(f"‚úì Score range: {heat_vuln.min():.1f} - {heat_vuln.max():.1f}")
print(f"‚úì Unique values: {heat_vuln.nunique()}")

# Overall assessment
issues_found = 0
recommendations = []

if perfect_scores > len(heat_vuln) * 0.1:
    issues_found += 1
    recommendations.append("High proportion of perfect scores suggests artificial construction")

if heat_vuln.nunique() < 20:
    issues_found += 1
    recommendations.append("Very few unique values indicate discrete/artificial scoring")

high_biomarker_corr = [x for x in biomarker_correlations if abs(x[1]) > 0.5]
if high_biomarker_corr:
    issues_found += 1
    recommendations.append(f"High correlations with {len(high_biomarker_corr)} biomarkers may indicate confounding")

very_high_component_corr = [x for x in component_correlations if x[1] > 0.9]
if very_high_component_corr:
    issues_found += 1
    recommendations.append("Near-perfect correlations with components suggest possible data leakage")

print(f"\nüîç ASSESSMENT:")
if issues_found == 0:
    print("‚úÖ HEAT_VULNERABILITY_SCORE appears to be a valid, well-constructed index")
    print("   ‚Üí ML dominance is likely due to genuine predictive power")
elif issues_found <= 2:
    print("‚ö†Ô∏è  HEAT_VULNERABILITY_SCORE shows some concerning patterns")
    print("   ‚Üí ML dominance may be partially due to construction artifacts")
else:
    print("üö® HEAT_VULNERABILITY_SCORE shows multiple concerning patterns")
    print("   ‚Üí ML dominance likely due to confounding/data leakage")

print(f"\nüìã RECOMMENDATIONS:")
for i, rec in enumerate(recommendations, 1):
    print(f"{i}. {rec}")

if not recommendations:
    print("1. Proceed with HEAT_VULNERABILITY_SCORE as primary predictor")
    print("2. Document its construction methodology")
    print("3. Validate findings with external datasets")
else:
    print(f"{len(recommendations)+1}. Investigate HEAT_VULNERABILITY_SCORE construction methodology")
    print(f"{len(recommendations)+2}. Create models excluding HEAT_VULNERABILITY_SCORE for comparison")
    print(f"{len(recommendations)+3}. Validate with independent heat vulnerability measures")

print(f"\nüå°Ô∏è HEAT-HEALTH RELATIONSHIP VALIDITY:")

# Final assessment based on DLNM results
if len(high_biomarker_corr) > 0 and issues_found <= 1:
    print("‚úÖ Strong correlations with biomarkers + temporal validation support genuine heat-health effects")
elif issues_found <= 2:
    print("‚ö†Ô∏è  Mixed evidence - some genuine effects but potential confounding")
else:
    print("‚ùå Multiple issues suggest relationships may be largely artifactual")

print(f"\n‚úì HEAT_VULNERABILITY_SCORE investigation completed")